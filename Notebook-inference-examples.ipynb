{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Bert Inference Notebook </center></h1>\n",
    "<h4><center>Final Project W266</center></h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\",\",\",\",O\r\n",
      "M.D,NNP,O\r\n",
      "JA25,NNP,O\r\n",
      "Attending:,NNP,O\r\n",
      "SYDNEY,NNP,O\r\n",
      "DUESTERHAUS,NNP,O\r\n",
      "\",\",\",\",O\r\n",
      "M.D,NNP,O\r\n",
      "MG85,NNP,O\r\n",
      "EQ681/3978,NNP,O\r\n",
      "Batch:,NNP,O\r\n",
      "37609,CD,O\r\n",
      "Index,NNP,O\r\n",
      "No,NNP,O\r\n",
      "FHOW8875S8,NNP,O\r\n",
      "D:,NNP,O\r\n",
      "6/10,CD,O\r\n",
      "T:,NNP,O\r\n",
      "1/22,CD,O\r\n",
      "[report_end],NN,O\r\n"
     ]
    }
   ],
   "source": [
    "!tail -20 'ner_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Getting Started<a id=\"start\" />\n",
    "\n",
    "We start with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval[gpu] in /opt/conda/lib/python3.6/site-packages (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from seqeval[gpu]) (1.16.3)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /opt/conda/lib/python3.6/site-packages (from seqeval[gpu]) (2.2.4)\n",
      "Requirement already satisfied: tensorflow-gpu; extra == \"gpu\" in /opt/conda/lib/python3.6/site-packages (from seqeval[gpu]) (1.13.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval[gpu]) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval[gpu]) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval[gpu]) (5.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval[gpu]) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval[gpu]) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval[gpu]) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (0.33.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (1.21.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (0.8.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (1.13.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (1.13.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (0.15.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (41.0.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu; extra == \"gpu\"->seqeval[gpu]) (3.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import keras\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model, model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU, Bidirectional\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "    def build_char_2_id_dict(self, data_set_char, min_freq):\n",
    "        char_freq = defaultdict(int)\n",
    "        char_2_id_table = {}\n",
    "\n",
    "        for char in [char for label, seq in data_set_char for char in seq]:\n",
    "            char_freq[char] += 1\n",
    "\n",
    "        id_counter = 1\n",
    "\n",
    "        for k, v in [(k, v) for k, v in char_freq.items() if v >= min_freq]:\n",
    "            char_2_id_table[k] = id_counter\n",
    "            id_counter += 1\n",
    "\n",
    "        return char_2_id_table\n",
    "\n",
    "    def build_data_set(self, data_set_char, char_2_id_dict, window_size):\n",
    "\n",
    "        data_set = []\n",
    "\n",
    "        for label, char_sequence in data_set_char:\n",
    "            ids = []\n",
    "\n",
    "            if len(char_sequence) == 2 * window_size + 1:\n",
    "                for char in char_sequence:\n",
    "                    if char in char_2_id_dict:\n",
    "                        ids.append(char_2_id_dict[char])\n",
    "                    else:\n",
    "                        ids.append(0)\n",
    "\n",
    "                feature_vector = np.array([float(ids[i])\n",
    "                                           for i in range(0, len(ids))], dtype=float)\n",
    "\n",
    "                data_set.append((float(label), feature_vector))\n",
    "\n",
    "        return data_set\n",
    "\n",
    "    def build_data_set_char(self, t, window_size):\n",
    "        data_set_char_eos = \\\n",
    "            [(1.0, t[m.start() - window_size:m.start()].replace(\"\\n\", \" \") +\n",
    "              t[m.start():m.start() + window_size + 1].replace(\"\\n\", \" \"))\n",
    "             for m in re.finditer('[\\.:?!;~][^\\n]?[\\n]', t)]\n",
    "\n",
    "        data_set_char_neos = \\\n",
    "            [(0.0, t[m.start() - window_size:m.start()].replace(\"\\n\", \" \") +\n",
    "              t[m.start():m.start() + window_size + 1].replace(\"\\n\", \" \"))\n",
    "             for m in re.finditer('[\\.:?!;~][^\\s]?[ ]+', t)]\n",
    "\n",
    "        return data_set_char_eos + data_set_char_neos\n",
    "\n",
    "    def build_potential_eos_list(self, t, window_size):\n",
    "        PUNCT = '[\\(\\)\\u0093\\u0094`“”\\\"›〈⟨〈<‹»«‘’–\\'``'']*'\n",
    "        EOS = '([\\.:?!;~])'\n",
    "\n",
    "        eos_positions = [(m.start())\n",
    "                         for m in re.finditer(r'([\\.:?!;~])(\\s+' + PUNCT + '|' +\n",
    "                                              PUNCT + '\\s+|[\\s\\n]+)', t)]\n",
    "        potential_eos_position = []\n",
    "\n",
    "        for eos_position in eos_positions:\n",
    "            left_context = t[eos_position - (2 * window_size):eos_position]\n",
    "            right_context = t[eos_position:eos_position + (3 * window_size)]\n",
    "\n",
    "            cleaned_left_context = left_context\n",
    "            cleaned_right_context = right_context\n",
    "\n",
    "            cleaned_left_context = re.sub('\\s+', ' ', cleaned_left_context)\n",
    "            cleaned_right_context = re.sub('\\s+', ' ', cleaned_right_context)\n",
    "\n",
    "            potential_eos_position.append((eos_position,\n",
    "                                          cleaned_left_context[-window_size:] + t[eos_position] +\n",
    "                                          cleaned_right_context[1:window_size + 1]))\n",
    "\n",
    "        return potential_eos_position\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('sentence_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"sentence_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define mddaximal length of input 'sentences' (post tokenization).\n",
    "max_word = 40\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = '/root/biobert_v1.0_pubmed_pmc/vocab.txt'\n",
    "MODEL = '/root/biobert_v1.0_pubmed_pmc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model2 = torch.load('/root/biobert.bin', map_location=lambda storage, loc: storage)\n",
    "model2 = model2.cpu().double()\n",
    "model2.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/root/biobert_v1.0_pubmed_pmc', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tag', 'cat']\n",
    "\n",
    "nerDistribution = pd.read_csv('ner_tags', sep = '\\t', names = columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-do</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-du</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-m</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-mo</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-r</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-do</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-du</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-f</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-m</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-mo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-r</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>O</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nerX</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag  cat\n",
       "0       B-do    0\n",
       "1       B-du    1\n",
       "2        B-f    2\n",
       "3        B-m    3\n",
       "4       B-mo    4\n",
       "5        B-r    5\n",
       "6       I-do    6\n",
       "7       I-du    7\n",
       "8        I-f    8\n",
       "9        I-m    9\n",
       "10      I-mo   10\n",
       "11       I-r   11\n",
       "12         O   12\n",
       "13  [nerCLS]   13\n",
       "14  [nerPAD]   14\n",
       "15  [nerSEP]   15\n",
       "16      nerX   16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "class ConnectionEngine:\n",
    "    def __init__(self, class_list):\n",
    "        self.class_list = class_list\n",
    "        self._connected = list()\n",
    "        self._connected.append(self._build_empty_row())\n",
    "\n",
    "    def _build_empty_row(self):\n",
    "        row = {k : None for k in self.class_list}\n",
    "        return row\n",
    "    \n",
    "    def add_item(self, class_name, value):\n",
    "        current_item = self._connected[-1]\n",
    "        if current_item[class_name] == None:\n",
    "            current_item[class_name] = value\n",
    "        else:\n",
    "            self._connected.append(self._build_empty_row())\n",
    "            current_item = self._connected[-1]\n",
    "            current_item[class_name] = value\n",
    "        return\n",
    "    \n",
    "    def get_connected(self):\n",
    "        return self._connected\n",
    "            \n",
    "    def print_out(self):\n",
    "        for item in self._connected:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(sent, label):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    new_label = []\n",
    "    for i, tok in enumerate(zip(sent,label)):\n",
    "        if tok[0].startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[0][2:]\n",
    "        else:\n",
    "            new_sent.append(tok[0])\n",
    "            new_label.append(tok[1])\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_label(sent, label):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    new_label = []\n",
    "    for i, tok in enumerate(zip(sent,label)):\n",
    "        if tok[0].startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[0][2:]\n",
    "        else:\n",
    "            new_sent.append(tok[0])\n",
    "            new_label.append(tok[1])\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_entities(sent, label):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    new_label = []\n",
    "    for i, tok in enumerate(zip(sent,label)):\n",
    "        if tok[1].startswith(\"nerX\") and not tok[0].startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[0]\n",
    "            new_label[len(new_label) - 1] = new_label[len(new_label) - 1] + ''\n",
    "        elif tok[1].startswith(\"nerX\") and tok[0].startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[0][2:]\n",
    "            new_label[len(new_label) - 1] = new_label[len(new_label) - 1] + ''\n",
    "        elif tok[0].startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[0][2:]\n",
    "            new_label[len(new_label) - 1] = new_label[len(new_label) - 1] + ''\n",
    "        else:\n",
    "            new_sent.append(tok[0])\n",
    "            new_label.append(tok[1])\n",
    "    return list(new_sent),list(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(sent, label, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent, label)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer_label(sent, label, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize_label(sent, label)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer_entities(sent, label, should_detokenize=True):\n",
    "    sent = []\n",
    "    label = []\n",
    "    if should_detokenize:\n",
    "        sent, label = detokenize_entities(sent, label)\n",
    "    print(\" \".join(sent))\n",
    "    print(\" \".join(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file='data/combined.txt'\n",
    "window_size=6\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "\n",
    "\n",
    "with open(training_file, mode='r', encoding='utf-8') as f:\n",
    "    training_corpus = f.read()\n",
    "\n",
    "data_set_char = util.build_data_set_char(\n",
    "    training_corpus, window_size)\n",
    "char_2_id_dict = util.build_char_2_id_dict(\n",
    "    data_set_char, min_freq)\n",
    "\n",
    "#with open('data/test.txt', mode='r', encoding='utf-8') as f:\n",
    "#    t = f.read()\n",
    "t = 'The patient was given Plavix ( clopidogrel ) 75 mg po daily. Advised ATENOLAL 50 mg po weekly for the patient'\n",
    "eos_marker=\"</eos>\"\n",
    "\n",
    "\n",
    "potential_eos_list = util.build_potential_eos_list(t, window_size)\n",
    "\n",
    "eos_counter = 0\n",
    "\n",
    "for potential_eos in potential_eos_list:\n",
    "    start, char_sequence = potential_eos\n",
    "\n",
    "    data_set = util.build_data_set([(-1.0, char_sequence)],\n",
    "                                        char_2_id_dict,\n",
    "                                        window_size)\n",
    "\n",
    "    if len(data_set) > 0:\n",
    "        label, feature_vector = data_set[0]\n",
    "\n",
    "        predicted = model.predict(\n",
    "            feature_vector.reshape(\n",
    "                1,\n",
    "                2 * window_size + 1),\n",
    "            batch_size=batch_size,\n",
    "            verbose=0)\n",
    "\n",
    "        if predicted[0][0] >= .4:\n",
    "            t = t[:(eos_counter * len(eos_marker)) + start + 1] + \\\n",
    "                eos_marker + t[(eos_counter * len(eos_marker)) + start + 1:]\n",
    "            eos_counter += 1\n",
    "\n",
    "blocked_text = t[:] + eos_marker\n",
    "sent_list = blocked_text.split('</eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence):\n",
    "    \n",
    "    tokenized_text_stg = [tokenizer.tokenize(sent) for sent in sentence.split()]\n",
    "    tokenized_text = [y for x in tokenized_text_stg for y in x]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(tokenized_text)],\n",
    "                          maxlen=64, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "    \n",
    "    b_input_ids = torch.tensor(input_ids)\n",
    "    b_attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "\n",
    "    logits = model2(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_attention_masks)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions = ([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    pred_tags = np.array([nerDistribution.loc[(nerDistribution['cat'] == p_i).idxmax()][0] for p in predictions for p_i in p])\n",
    "    sent = []\n",
    "    label = []\n",
    "    sent, label = detokenize_entities(tokenized_text, list(pred_tags))\n",
    "    return sent, label\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['The', 'patient', 'was', 'given', 'Plavix', '(', 'clopidogrel', ')', '75', 'mg', 'po', 'daily.'], ['O', 'O', 'O', 'O', 'B-m', 'I-m', 'I-m', 'I-m', 'B-do', 'I-do', 'B-mo', 'B-f'])\n",
      "(['Advised', 'ATENOLAL', '50', 'mg', 'po', 'weekly', 'for', 'the', 'patient'], ['O', 'B-m', 'B-do', 'I-do', 'B-mo', 'B-f', 'O', 'O', 'O'])\n",
      "([], [])\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sent_list):\n",
    "    #print(sentence)\n",
    "    print(inference(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
